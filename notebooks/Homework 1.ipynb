{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning Algorithm (PLA)\n",
    "\n",
    "## Learning from Data: Homework 1\n",
    "\n",
    "The following questions from this homework will be answered and its solutions will be explained: questions 7, 8, 9 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "In this problem, you will create your own target function $f$ and data set $D$ to see how the Perceptron Learning Algorithm works. Take $d = 2$ so you can visualize the problem, and assume $\\mathcal{X} = [ 1, 1] \\times [ 1, 1]$ with uniform probability of picking each $x \\in \\mathcal{X}$.\n",
    "\n",
    "In each run, choose a random line in the plane as your target function $f$ (do this by taking two random, uniformly distributed points in $[ 1, 1] \\times [ 1, 1]$ and taking the line passing through them), where one side of the line maps to $+1$ and the other maps to $-1$. Choose the inputs $x_n$ of the data set as random points (uniformly in $\\mathcal{X}$), and evaluate the target function on each $x_n$ to get the corresponding output $y_n \\in \\mathcal{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "with open('dataset.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a random target function\n",
    "def generate_target():\n",
    "    p = np.random.uniform(-1, 1, (2,2))\n",
    "\n",
    "    x = [[z,1] for z in p[:,0]]\n",
    "    y = p[:,1]\n",
    "    a,b = np.linalg.solve(x,y)\n",
    "    \n",
    "    w = [-b, -a, 1]\n",
    "    \n",
    "    Ys = np.sign(np.dot(dataset, w))\n",
    "    return (w, Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick a random data set of the required size\n",
    "def pick_data(targets, N, dim):\n",
    "    Xs = np.zeros((N,dim))\n",
    "    Ys = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        idx = np.random.randint(0, len(dataset))\n",
    "        Xs[i] = dataset[idx]\n",
    "        Ys[i] = targets[idx]\n",
    "    return (Xs, Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Learning Algorithm (PLA):\n",
    "\n",
    "Consider a hypothesis set $\\mathcal{H}$, where each hypothesis $h \\in \\mathcal{H}$ has a functional form. This functional form gives different weights to the different coordinates of the input vector $x$, reflecting their relative importance. These weighted coordinates are then combinated and compared to a threshold value. If the resulting value is above or below the threshold, a positive, respectively, negative output is returned. The function form of the hypothesis $h$ can be written as:\n",
    "\n",
    "$$h(x) = sign \\left( \\left( \\sum_{i=1}^{d}{w_i x_i} \\right) + b \\right)$$\n",
    "\n",
    "where $x_1, \\ldots, x_d$ are the components of the input vector $x$ and $sign(s) = +1$, if $s > 0$, while $sign(s) = -1$, if $s < 0$. The weights which are uses to weigh the coordinates are $w_1, \\ldots, w_d$ and the threshold is determined by the term $b$. This model of the hypothesis set $\\mathcal{H}$ is called the _perceptron_.\n",
    "\n",
    "The learning algorithm used to search the hypothesis space $\\mathcal{H}$ will look for weights and a bias term that performs well on the data set. Some of the weights might become negative, indicating an adverse effect for the corresponding components of the input vector $x$, while other weights can become positive. The optimal choices of the weight vector and the bias term define the final hypothesis $g \\in \\mathcal{H}$ that the algorithm produces. However, this final hypothesis $g$ will only be able to classify all training examples correctly when the data set is _linearly separable_.\n",
    "\n",
    "To enable vector notation, we will consider the bias term $b$ as a weight $w_0 = b$. So the weight vector $w$ becomes $w = [w_0, w_1, \\ldots, w_d]$. Similarly, the input vector $x$ is extended with the fixed value $x_0 = 1$. So, $x$ becomes $x = [x_0, x_1, \\ldots, x_d]$. With this notation, the hypothesis $h$ can be written as\n",
    "\n",
    "$$h(x) = sign(w^T x)$$\n",
    "\n",
    "The Perceptron Learning Algorithm (PLA) will determine the weight vector $w$, based on the data. Assuming the given data is linearly separable, there exists a weight vector $w$ such that the hypothesis $h$ classifies all training points correctly, i.e. $h(x_n) = y_n$ for all $n$. This is done by a simple iterative method. At iteration $t$, where $t=0,1,2,\\ldots$ the current value of the weight vector $w$ is denoted $w(t)$. The algorithm picks a misclassified example, denoted $(x(t), y(t))$, and uses this point to update $w(t)$. Since the chosen example is misclassified, we have $y(t) \\ne sign(w^T(t) x(t))$. The update rule is\n",
    "\n",
    "$$w(t+1) = w(t) + y(t)x(t)$$\n",
    "\n",
    "This rule moves the boundary that classifies the points in the direction of classifying the point $x(t)$ correctly. This iterative procedure is repeated until there are no more misclassified examples in the data set.\n",
    "\n",
    "Although the update rule considers only one training example at a time, this algorithm is indeed guaranteed to arrive at the correct solution (given the data is linearly separable). This holds regardless of which misclassified example $(x(t), y(t))$ is chosen and regardless of how the weight vector $w$ is initialized.\n",
    "\n",
    "The code below, more specfically the **PLA** function, implementes this Perceptron Learning Algorithm. The arguments for this function are the amount of iterations to perform, the size of the dataset $N$ and the dimension of the dataset. The algorithm starts by first generating a random target function (**generate_target** function). This returns the weight vector $w$ and the correct outputs $\\mathcal{Y}$ for all data points in the dataset. Next, a random subset of size $N$ of the data is chosen and the weight vector $w_{est}$ that will be learned is filled with zeros. Then the algorithm's main loop begins.\n",
    "\n",
    "In every iteration of this main loop, the following actions are done; First, the hypothesis $g$ is calculated as specified above, namely $sign(w_{est}^T, x)$. Next, all missclassified data points (or $x$'s) and their corresponding output values (or $y$'s) are collected in the **wrong_Xs** and **wrong_Ys** arrays respectively. These will be usefull for implementing the update rule. The accuracy is determined by counting the number of correctly classified examples. If not all points are correctly classified, a random missclassified point is chosen. For this, the two arrays mentioned above are used. This randomly chosen missclassified point is used to execute the PLA update rule, namely $w_{est} = w_{est} + yx$.\n",
    "\n",
    "In order to determine the value for $\\mathbb{P}[f(x) \\ne g(x)]$, or the probability that $f$ and $g$ will disagree on their classification of a random point, the function **error_rate** is used. This function will take as input the actual weight vector $w$ from the target function and the learned weight vector $w_{est}$ and compute the output values for the entire dataset, once for each of these weight vectors. The error rate is then determined by counting the amount of missclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rate(w, w_est):\n",
    "    actual_Ys = np.sign(np.dot(dataset, w))\n",
    "    predicted_Ys = np.sign(np.dot(dataset, w_est))\n",
    "\n",
    "    return np.mean(actual_Ys != predicted_Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main PLA Algorithm\n",
    "def PLA(iterations, N, dim):\n",
    "    convergence = np.zeros(iterations)\n",
    "    error_rates = np.zeros(iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        w, Y = generate_target()\n",
    "        Xs, Ys = pick_data(Y, N, dim)\n",
    "        w_est = np.zeros(dim)\n",
    "        ctr = 0\n",
    "        acc = 0\n",
    "        while acc < 1:\n",
    "            ctr += 1\n",
    "            # Make and evaluate predictions\n",
    "            pred = np.sign(np.dot(Xs, w_est))\n",
    "            wrong_Xs = Xs[pred != Ys]\n",
    "            wrong_Ys = Ys[pred != Ys]\n",
    "        \n",
    "            # Measure accuracy of current iteration\n",
    "            acc = np.mean(pred == Ys)\n",
    "        \n",
    "            # Randomly pick one misclassified point and update weight\n",
    "            if acc < 1:\n",
    "                idx = np.random.randint(len(wrong_Xs))\n",
    "                w_est = w_est + wrong_Ys[idx] * wrong_Xs[idx]        \n",
    "        convergence[i] = ctr\n",
    "        error_rates[i] = error_rate(w, w_est)\n",
    "    return (np.mean(convergence), np.mean(error_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the PLA algorithm for X iterations, with Y data points\n",
    "def main(iterations, N):\n",
    "    dim = dataset.shape[1]\n",
    "    conv, err_rate = PLA(iterations, N, dim)\n",
    "    return (conv, err_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 7\n",
    "\n",
    "How many iterations does it take on average for the PLA to converge for $N = 10$ training points?\n",
    "\n",
    "### Question 8\n",
    "\n",
    "Which of the following is closest to $\\mathbb{P}[f(x) \\ne g(x)]$ for $N = 10$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLA run for 1000 iterations\n",
      "Dataset of size 10\n",
      "Average error rate is 0.107813\n",
      "Average number of iterations needed for convergence is 10.087\n"
     ]
    }
   ],
   "source": [
    "conv, err_rate = main(1000,10)\n",
    "print(\"PLA run for 1000 iterations\")\n",
    "print(\"Dataset of size 10\")\n",
    "print(\"Average error rate is\", err_rate)\n",
    "print(\"Average number of iterations needed for convergence is\", conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of iterations needed to reach convergence is closest to the one of answer B ($15$).\n",
    "\n",
    "The average error rate is closest to the one of answer C ($0.1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 9\n",
    "\n",
    "How many iterations does it take on average for the PLA to converge for $N = 100$ training points?\n",
    "\n",
    "### Question 10\n",
    "\n",
    "Which of the following is closest to $\\mathbb{P}[f(x) \\ne g(x)]$ for $N = 100$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLA run for 1000 iterations\n",
      "Dataset of size 100\n",
      "Average number of iterations needed for convergence is 94.485\n",
      "Average error rate is 0.012999\n"
     ]
    }
   ],
   "source": [
    "conv, err_rate = main(1000,100)\n",
    "print(\"PLA run for 1000 iterations\")\n",
    "print(\"Dataset of size 100\")\n",
    "print(\"Average number of iterations needed for convergence is\", conv)\n",
    "print(\"Average error rate is\", err_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The average number of iterations needed to reach convergence is closest to the one of answer B ($100$).\n",
    "\n",
    "The average error rate is closest to the one of answer B ($0.01$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
