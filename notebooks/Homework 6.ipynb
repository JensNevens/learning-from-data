{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Regularization with Weight Decay\n",
    "\n",
    "## Learning from Data: Homework 6\n",
    "\n",
    "The following questions from this homework will be answered and its solutions will be explained: questions 2, 3, 4, 5 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The following problems use [this training set](http://work.caltech.edu/data/in.dta) and [this test set](http://work.caltech.edu/data/out.dta).\n",
    "\n",
    "Each line of the files corresponds to a two-dimensional input $x = (x_1,x_2)$, so that $\\mathcal{X} = \\mathbb{R}^2$, followed by the corresponding label from $\\mathcal{Y} = \\{-1, 1\\}$. We are going to apply Linear Regression with a non-linear transformation for classification. The nonlinear transformation is given by\n",
    "\n",
    "$$\\phi(x_1, x_2) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, |x_1 - x_2|, |x_1 + x_2|)$$\n",
    "\n",
    "\n",
    "Recall that the classification error is defined as the fraction of missclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"in.dta\", sep=\" +\", header=None, engine='python')\n",
    "train.columns = [\"x1\", \"x2\", \"y\"]\n",
    "test = pd.read_table(\"out.dta\", sep=\" +\", header=None, engine='python')\n",
    "test.columns = [\"x1\", \"x2\", \"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df['one'] = 1\n",
    "    df['x1^2'] = df['x1']**2\n",
    "    df['x2^2'] = df['x2']**2\n",
    "    df['x1x2'] = df['x1']*df['x2']\n",
    "    df['|x1-x2|'] = np.abs(df['x1'] - df['x2'])\n",
    "    df['|x1+x2|'] = np.abs(df['x1'] + df['x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.reindex(columns = ['one', 'x1', 'x2', 'x1^2', 'x2^2', 'x1x2', '|x1-x2|', '|x1+x2|', 'y'])\n",
    "test = test.reindex(columns = ['one', 'x1', 'x2', 'x1^2', 'x2^2', 'x1x2', '|x1-x2|', '|x1+x2|', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>one</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x1^2</th>\n",
       "      <th>x2^2</th>\n",
       "      <th>x1x2</th>\n",
       "      <th>|x1-x2|</th>\n",
       "      <th>|x1+x2|</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.779470</td>\n",
       "      <td>0.838221</td>\n",
       "      <td>0.607574</td>\n",
       "      <td>0.702615</td>\n",
       "      <td>-0.653369</td>\n",
       "      <td>1.617692</td>\n",
       "      <td>0.058751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.155635</td>\n",
       "      <td>0.895377</td>\n",
       "      <td>0.024222</td>\n",
       "      <td>0.801701</td>\n",
       "      <td>0.139352</td>\n",
       "      <td>0.739743</td>\n",
       "      <td>1.051012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.059908</td>\n",
       "      <td>-0.717780</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>0.515208</td>\n",
       "      <td>0.043001</td>\n",
       "      <td>0.657872</td>\n",
       "      <td>0.777688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.207596</td>\n",
       "      <td>0.758933</td>\n",
       "      <td>0.043096</td>\n",
       "      <td>0.575980</td>\n",
       "      <td>0.157552</td>\n",
       "      <td>0.551337</td>\n",
       "      <td>0.966530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.195983</td>\n",
       "      <td>-0.375487</td>\n",
       "      <td>0.038409</td>\n",
       "      <td>0.140991</td>\n",
       "      <td>0.073589</td>\n",
       "      <td>0.179504</td>\n",
       "      <td>0.571470</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   one        x1        x2      x1^2      x2^2      x1x2   |x1-x2|   |x1+x2|  \\\n",
       "0    1 -0.779470  0.838221  0.607574  0.702615 -0.653369  1.617692  0.058751   \n",
       "1    1  0.155635  0.895377  0.024222  0.801701  0.139352  0.739743  1.051012   \n",
       "2    1 -0.059908 -0.717780  0.003589  0.515208  0.043001  0.657872  0.777688   \n",
       "3    1  0.207596  0.758933  0.043096  0.575980  0.157552  0.551337  0.966530   \n",
       "4    1 -0.195983 -0.375487  0.038409  0.140991  0.073589  0.179504  0.571470   \n",
       "\n",
       "   y  \n",
       "0  1  \n",
       "1  1  \n",
       "2  1  \n",
       "3  1  \n",
       "4 -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we explain Linear Regression and non-linear transformations:\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Linear Regression is a linear model that applies to real-values target functions. So, we have a data set $\\mathcal{D}$ of training examples $(x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)$ where $x_n$ are the input values and $y_n$ are the real-valued output values or targets. Our goal is to find a hypothesis $g$ that minimizes the amount of misclassified points.\n",
    "\n",
    "Since our targets are now in the real domain, they will no longer be a deterministic function $y=f(x)$. Instead, it will be a noisy target function formalized as a distribution of the random variable $y$. More precisely, the label $y_n$ comes from some distribution $P(y|x)$. So, we have an unknown distribution $P(x,y)$ that generates our training examples $(x_n, y_n)$ and we want to find a hypothesis $g$ that minimizes the error between $g(x)$ and $y$ w.r.t. that distribution. The assumption made here is that there exists a linear combination of the input values $x$ that properly approximates the output values $y$. If this assumption does not hold, we can opt for a non-linear transformation, which will be discussed later on.\n",
    "\n",
    "The linear regression algorithm tries to minimize the out-of-sample error, $E_{out}(h)$, by minimizing the squared error between the hypothesis $h(x)$ and the output $y$\n",
    "\n",
    "$$E_{out}(h) = \\mathbb{E} \\left[(h(x)-y)^2 \\right]$$\n",
    "\n",
    "where the expected value is taken w.r.t. the probability distribution $P(x,y)$. Since the distribution $P(x,y)$ is unknown, $E_{out}(h)$ can not be computed. So, we must use the in-sample error instead:\n",
    "\n",
    "$$E_{in}(h) = \\frac{1}{N} \\sum_{n=1}^{N}{\\left( h(x_n) - y_n \\right)^2}$$\n",
    "\n",
    "As was already mentioned, in linear regression, the hypothesis $h$ takes the form of a linear combination of the components of the inputs $x$. Formally:\n",
    "\n",
    "$$h(x) = \\sum_{i=0}^{d}{w_i x_i} = w^Tx$$\n",
    "\n",
    "where $x_0 = 1$. For linear regression, it is very useful to represent the inputs and outputs as matrices. The data matrix $X \\in \\mathbb{R}^{N \\times (d+1)}$ is the $N \\times (d+1)$ matrix whose rows are the inputs $x_n$ as row vectors. The target vector $y \\in \\mathbb{R}^N$ is the column vector whose components are the target values $y_n$. This can be summarized as:\n",
    "\n",
    "$$X=\\left[ \\begin{matrix} x_{ 1 }^{ T } \\\\ x_{ 2 }^{ T } \\\\ \\vdots  \\\\ x_N^T \\end{matrix} \\right], \\qquad y=\\left[ \\begin{matrix} y_1 \\\\ y_2 \\\\ \\vdots  \\\\ y_N \\end{matrix} \\right] $$\n",
    "\n",
    "With this representation, the linear regression algorithm can be derived and consists of the following steps:\n",
    "\n",
    " 1. Construct the matrix $X$ and $y$ as explained above.\n",
    " 2. Compute the pseudo-inverse $X^{\\dagger}$ of the matrix $X$. If $X^TX$ is invertible, $$X^{\\dagger} = (X^TX)^{-1}X^T$$\n",
    " 3. Return $w_{lin} = X^{\\dagger}y$.\n",
    " \n",
    "This is exactly what the first code fragment below does. The function **lin_regression** computes the dot product of the pseudo-inverse of $X$ and $y$. The returned value is assigned to the weight vector $w$. The second function, **class_error**, computes the classification error by comparing the outputs found with our learned weight vector $w$ to the real outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear Transformation\n",
    "\n",
    "As mentioned, linear regression assumes a linear combination of the inputs $x$ to properly approximate the outputs $y$. This assumption is clear when looking at the form of the hypothesis $h$:\n",
    "\n",
    "$$h(x) = w^Tx$$\n",
    "\n",
    "This quantity is not only linear in terms of the $x_i$'s, but also in terms of the $w_i$'s. In fact, the linearity of the $w_i$'s is the only thing that matters, since these need to be learned. The $x_i$'s are merely constants. This observation allows us to apply non-linear transformations to the $x_i$'s, while still using the linear model as stated above, since the resulting model remains linear in terms of the $w_i$'s. Applying non-linear transformations to the inputs $x_i$ allows for more rich information contained within these inputs.\n",
    "\n",
    "A non-linear transformation on the inputs $x$ is usually denoted by the function $\\phi(x)$. This results in a vector $z$. While the inputs $x$ live in the $\\mathcal{X}$ space, the non-linearly transformed inputs $z$ live in the $\\mathcal{Z}$ space. The $\\mathcal{Z}$ space is also referred to as the feature space since it contains higher-level features derived from the raw inputs $x$. The transformation function $\\phi$ is called the feature transform.\n",
    "\n",
    "With these non-linear transformations, we can now represent a non-linear hypothesis $h$ in the $\\mathcal{X}$ space by a linear hypothesis $\\tilde{h}$ in the $\\mathcal{Z}$ space. More formally:\n",
    "\n",
    "$$h(x) = \\tilde{h}(\\phi(x))$$\n",
    "\n",
    "As was already mentioned in the exercise, the non-linear transformation $\\phi$ applied to the inputs $x$ is:\n",
    "\n",
    "$$\\phi(x_1, x_2) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, |x_1 - x_2|, |x_1 + x_2|)$$\n",
    "\n",
    "The code responsible for this is the 3th code block of this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lin_regression(Xs, Ys):\n",
    "    # Dot product of pseudo-inverse of X and Y\n",
    "    return np.dot(np.linalg.pinv(Xs), Ys)\n",
    "\n",
    "# One step learning: determine the weights\n",
    "w = lin_regression(train.ix[:,:-1], train['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classification error: Compute outputs with current w\n",
    "# and compare to real outputs\n",
    "def class_error(Xs, Ys, w):\n",
    "    Y_est = np.sign(np.dot(Xs, w))\n",
    "    return np.mean(Y_est != Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Run Linear Regression on the training set after performing the non-linear transformation and compute the in-sample and out-of-sample error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028571428571428571"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification error on training set\n",
    "class_error(train.ix[:,:-1], train['y'], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.084000000000000005"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classification error on test set\n",
    "class_error(test.ix[:,:-1], test['y'], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that these values are the closest to the one of answer A ($0.03$ and $0.08$).\n",
    "\n",
    "Both the in-sample and out-of-sample error are already small, indicating a good performance of the Linear Regression algorithm. It might be possible to improve this performance even further. Maybe this model slightly overfits the data? One way to combat overfitting is by using Regularized Linear Regression with Weight Decay. What these concepts are will be explained in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression with Weight Decay\n",
    "\n",
    "We can say a model overfits the data when it is able to produce very accuracte predictions on the training data, i.e. the seen data, but it does not generalize well to unseen data. In fact, the model is trained on the training data so much that is has learned to fit the noise in that data. So, one can notice overfitting has occured when picking a hypothesis $g$ with a lower value for $E_{in}$ and getting a higher value for $E_{out}$.\n",
    "\n",
    "One way to deal with overfitting is to use Regularization. This technique will constraint the learning algorithm to improve the out-of-sample error. One way to write this down formally is as follows:\n",
    "\n",
    "$$E_{out}(h) \\le E_{in}(h) + \\Omega(\\mathcal{H}), \\qquad \\text{for all}~h \\in \\mathcal{H}$$\n",
    "\n",
    "where $\\Omega(\\mathcal{H})$ is a model complexity penalty. This tells us we will be better off by fitting the data using a simple hypothesis $h \\in \\mathcal{H}$. The difficulty of Regularization is finding a suitable complecity measure $\\Omega(\\mathcal{H})$. With Regularization, one not only minimizes $E_{in}$, but the combination of $E_{in}$ and $\\Omega(\\mathcal{H})$.\n",
    "\n",
    "One way of Regularization is called Weight Decay. First, we define the augmented error as:\n",
    "\n",
    "$$E_{aug}(w) = E_{in}(w) + \\lambda w^T w$$\n",
    "\n",
    "where $\\lambda \\ge 0$ is a free parameter. This augmented error has two terms: the in-sample error and the penalty term discussed earlier. The aim of Weight Decay is to minimize this penalized in-sample error. The value of $\\lambda$ controls the amount of regularization, while the penalty term $w^T w$ enforces a tradeoff between making the in-sample error small and making the weights small. With a technique like gradient descent e.g., we find a reducation in in-sample error together with a gradual shrinking of the weights. In fact, a more general definition of the augmented error can be given:\n",
    "\n",
    "$$E_{aug}(h, \\lambda, \\Omega) = E_{in}(h) + \\frac{\\lambda}{N} \\Omega(h)$$\n",
    "\n",
    "This defition is more suitable, since we are not only in control of the hypothesis $h$, but also over the amount of regularization $\\lambda$ and the penalty term $\\Omega(h)$, which we chose to be $w^T w$ for weight decay. The need for regularization goes down as the data set size $N$ increases, so $\\frac{1}{N}$ was left out in the previous equation. This allows for $\\lambda$ to be less sensitive to $N$.\n",
    "\n",
    "As was the case with the normal variant of Linear Regression described above, so is also Regularized Linear Regression a one-step-learning algorithm. It can be derived that for Regularized Linear Regression with Weight Decay the weight vector $w_{reg}$ can be calculated as follows:\n",
    "\n",
    "$$w_{reg} = (X^T X + \\lambda I)^{-1} X^T y$$\n",
    "\n",
    "In the code fragments below, we did not directly specify a value for the parameter $\\lambda$. Instead, we defined $\\lambda$ as being of the following form:\n",
    "\n",
    "$$\\lambda = 10^{k}$$\n",
    "\n",
    "and provided the parameter $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Regularized linear regression\n",
    "def regul_lin_regression(Xs, Ys, k):\n",
    "    l = 10**k\n",
    "    A = np.linalg.inv(np.dot(Xs.T, Xs) + np.eye(Xs.shape[1])*l)\n",
    "    B = np.dot(Xs.T, Ys)\n",
    "    return np.dot(A,B)\n",
    "\n",
    "# Weights with k = -3, i.e. lambda = 10E-3\n",
    "w_neg_k = regul_lin_regression(train.ix[:,:-1], train['y'], -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Add Weight Decay to the Linear Regression and compute the in-sample and out-of-sample error for $k=-3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.028571428571428571"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification error on train set\n",
    "#with k = -3, i.e. lambda = 10E-3\n",
    "class_error(train.ix[:,:-1], train['y'], w_neg_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080000000000000002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification error on test set\n",
    "#with k = -3, i.e. lambda = 10E-3\n",
    "class_error(test.ix[:,:-1], test['y'], w_neg_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the in- and out-of sample error are closest to the values of answer D ($0.03$ and $0.08$).\n",
    "\n",
    "As can be seen from these results, using a small amount of regularization ($\\lambda = 0.001$) did not improve the performance of the algorithm. The in-sample and out-of-sample errors are still te same. Maybe we should try more regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Add Weight Decay to the Linear Regression and compute the in-sample and out-of-sample error for $k=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weights with k = 3, i.e. lambda = 10E3\n",
    "w_pos_k = regul_lin_regression(train.ix[:,:-1], train['y'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37142857142857144"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification error on train set\n",
    "#with k = 3, i.e. lambda = 10E3\n",
    "class_error(train.ix[:,:-1], train['y'], w_pos_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.436"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification error on test set\n",
    "#with k = 3, i.e. lambda = 10E3\n",
    "class_error(test.ix[:,:-1], test['y'], w_pos_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the in- and out-of sample errors when using $k=3$ are closest to the ones of answer E ($0.4$ and $0.4$).\n",
    "\n",
    "As can be seen from these results, applying too much regularization ($\\lambda = 1000$) worsens the algorithm's performance. In the following section, a good value for $k$ will be searched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Add Weight Decay to the Linear Regression and see which value of $k$ produces the smalles in-sample and out-of-sample error for $k=[2,1,0,-1,-2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find minimum E_out\n",
    "def min_E_out(ks):\n",
    "    errors = np.zeros(len(ks))\n",
    "    for i,k in enumerate(ks):\n",
    "        w_est = regul_lin_regression(train.ix[:, :-1], train['y'], k)\n",
    "        error = class_error(test.ix[:,:-1], test['y'], w_est)\n",
    "        errors[i] = error\n",
    "    min_k = np.argmin(errors)\n",
    "    return (errors, errors[min_k], ks[min_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.228,  0.124,  0.092,  0.056,  0.228]), 0.056000000000000001, -1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vary k over [2,1,0,-1,-2]\n",
    "min_E_out([2,1,0,-1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest out-of-sample classification error is achieved when we use $k=-1$, which is answer D.\n",
    "\n",
    "From these results, it can be seen that the minimal out-of-sample error ($E_{out} = 0.056$) is found when $k=-1$, or $\\lambda = 0.1$. So the ideal amount of regularization was in-between what we tried earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Add Weight Decay to the Linear Regression and see which value of $k$ produces the smalles in-sample and out-of-sample error, limiting $k$ to integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.084,  0.084,  0.084,  0.084,  0.084,  0.084,  0.084,  0.08 ,\n",
       "         0.084,  0.056,  0.092,  0.124,  0.228,  0.436,  0.452,  0.456,\n",
       "         0.456,  0.456,  0.456,  0.456,  0.456]), 0.056000000000000001, -1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vary k over range [-10, 10]\n",
    "min_E_out(np.arange(-10,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The value closest to the minimum out-of-sample classification error achieved by varying $k$ (limiting $k$ to integer values) is $k=0.06$, which is answer B.\n",
    "\n",
    "Again, we see the optimal value for $\\lambda$ is $\\lambda = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
