{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and RBF Kernel\n",
    "\n",
    "## Learning from Data: Homework 8\n",
    "\n",
    "For this homework, the library _libsvm_ is needed. The easiest way to install this on _Mac OS X_ is using [_Homebrew_](http://brew.sh/) with the following command: `brew install libsvm`.\n",
    "\n",
    "The following questions from this homework will be answered and its solutions will be explained: questions 7, 8, 9 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial kernel\n",
    "In the first 2 questions, we need to experiment with 10-fold cross-validation using a polynomial kernel. We will first explain the workings of support vector machines, after which we will look at the use of a polynomial kernel. Afterwards, we see why cross-validation is useful and how it works.\n",
    "\n",
    "The purpose of support vector machines is to find dichotomies with border margins that are as large as possible.\n",
    "As we saw in the lectures, we first need the distance between the hyperplane $w^Tx=0$ and the nearest point to that hyperplane, in which $w$ are weights and $x$ are data points. This was found to be $\\frac{1}{\\|w\\|}$.\n",
    "This is the value that needs to be maximized in order to get a margin as big as possible. This can be restated by saying that we need to minimize the value $\\frac{1}{2}w^\\intercal w$. This is allowed because minimizing this quantity maximizes the original quantity. <br />\n",
    "The optimization should also be subject to $|w^\\intercal x_n + b| \\geq 1$ (in which $b$ is the bias) for $n=1,2,\\dots,N$, because the distance from the separating hyperplane to its nearest point should be normalized to be 1. Eventually, we get the following Lagrangian formulation which incorporates both the quantity to minimize and the constraint:\n",
    "$$\n",
    "\\mathcal{L}(\\alpha) = \\sum_{n=1}^N \\alpha_n - \\frac{1}{2} \\sum_{n=1}^N \\sum_{m=1}^N y_ny_m \\alpha_n\\alpha_m x_n^\\intercal x_m\n",
    "$$\n",
    "In which $y_n$ and $y_m$ are target values and $\\alpha_n$ and $\\alpha_m$ are Lagrange multipliers.\n",
    "Using quadratic programming, we eventually become $\\alpha=\\alpha_1, \\alpha_2, \\dots, \\alpha_N$, which maximizes $\\frac{1}{2}w^\\intercal w$, while taking into account the constraint. When $\\alpha_n > 0$, we say that the corresponding data point $x_n$ is a support vector. The hyperplane that separates the points is completely determined by those support vectors and must lie between the support vectors of different target values.\n",
    "\n",
    "For data which is not linearly separable, a nonlinear transform is needed, in which a transformation (using the function $\\phi$) of the data is done from the $\\mathcal{X}$ to the $\\mathcal{Z}$ space. The resulting data in this $\\mathcal{Z}$ is then linearly separable.\n",
    "\n",
    "\n",
    "In a polynomial kernel, we use a nonlinear transformation function $\\phi: \\mathcal{X} \\rightarrow \\mathcal{Z}$ that is a polynomial of a certain order $Q$.\n",
    "It is possible to do this nonlinear transformation first and then apply quadratic programming using the transformed data to obtain our solution. This can however be infeasible when we have a high-order $\\phi$ function, because transforming all the data is computationally expensive. When we look at the Lagrange formulation however, we see that the training data is only used in a dot product of 2 such data points: $x_n^\\intercal x_m$. Thus, we actually only need the result of this dot product in the $\\mathcal{Z}$ space. The transformed data is not used in any other way. A function that takes 2 data points in the $\\mathcal{X}$ space and computes their inner product in the $\\mathcal{Z}$ space is called a kernel function. In a polynomial kernel, this function is $K(x,x')=(1+x^\\intercal x')^Q$. We can thus replace the inner product in the Lagrange formulation with this kernel function.\n",
    "\n",
    "\n",
    "There are however cases where the data is slightly non-separable but not so much that a non-linear transformation is needed, or the that is still non-separable after a transformation. As a result, some points violate the margin ($|w^\\intercal x_n + b| \\geq 1$ is violated). The total violation was found to be $\\sum_{n=1}^N \\xi_n$. An order of this value (the value $C$) is added to the value that needs to be minimized using quadratic programming in order to allow some errors. The higher $C$, the more errors are allowed in separating the data. Intuitively, we see that increasing $C$ makes the decision surface more smooth and simple. A margin that uses this $C$ is called a soft margin.\n",
    "\n",
    "\n",
    "For the first 2 exercises, we use cross validation. In the slides, we first saw that using small partition of the training set to validate the learned hypothesis leads to a bad estimate because the points taken out of the training set ($K$ points out of $N$) for validation may not be representative for estimating the out-of-sample error, $E_{out}$. When, we take a large partition however, we get again an accurate validation error ($E_{val}$), but because the model is learned from a small number of data points, we have a bigger chance of getting a bad model for the data. Thus, we need to balance $K$.\n",
    "\n",
    "The optimal situation however would be to have $K$ both small and large, thus getting a good model and a good estimation of $E_{out}$. To achieve this, we use points one time for validating, and other times for training a model.\n",
    "We separate the $N$ datapoints into a number of folds, $F$. These number of folds could be as large as $N$ itself, in which $N$ iterations will take place and when only one point is used for validation.\n",
    "In cross-validation, $K=\\frac{N}{F}$, but these K points differ each time as a different fold is used for validation.\n",
    "The total cross-validation error will then be $E_{cv}=\\frac{1}{F}\\sum_{n=1}^F e_n$, in which $e_n=E_{val}(g_n^-)$ and $g_n^-$ is the model trained on $N-K$ datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the exercises, we only take data with as target value (digit) 1 or 5 and replace this target values by respectively -1 and 1.\n",
    "After listing all the possible values on $C$ that need to be used with the support vector machine, we do a number of runs. In each run, we separate the training data randomly into 10 folds. Then, for each value of $C$, we iterate 10 times. Each time take a different fold on which the classifier will be tested and the other 9 folds to train the classifier. This classifier is made using a support vector machine which is given the current value of $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from svmutil import *\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import KFold\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "pd.options.mode.chained_assignment = None\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"features.train\", sep=\" +\", header=None, engine='python')\n",
    "train.columns = [\"digit\", \"intensity\", \"symmetry\"]\n",
    "test = pd.read_table(\"features.test\", sep=\" +\", header=None, engine='python')\n",
    "test.columns = [\"digit\", \"intensity\", \"symmetry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>digit</th>\n",
       "      <th>intensity</th>\n",
       "      <th>symmetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.341092</td>\n",
       "      <td>-4.528937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.444131</td>\n",
       "      <td>-5.496812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.231002</td>\n",
       "      <td>-2.886750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.200275</td>\n",
       "      <td>-3.534375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.291936</td>\n",
       "      <td>-4.352062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   digit  intensity  symmetry\n",
       "0      6   0.341092 -4.528937\n",
       "1      5   0.444131 -5.496812\n",
       "2      4   0.231002 -2.886750\n",
       "3      7   0.200275 -3.534375\n",
       "4      3   0.291936 -4.352062"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = train[(train.digit == 1) | (train.digit == 5)]\n",
    "filtered.loc[filtered.digit == 1, \"digit\"] = -1\n",
    "filtered.loc[filtered.digit == 5, \"digit\"] = 1\n",
    "filtered = filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>digit</th>\n",
       "      <th>intensity</th>\n",
       "      <th>symmetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.444131</td>\n",
       "      <td>-5.496812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.123043</td>\n",
       "      <td>-0.707875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.113859</td>\n",
       "      <td>-0.931375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.115371</td>\n",
       "      <td>-0.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.102281</td>\n",
       "      <td>-0.378812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   digit  intensity  symmetry\n",
       "0      1   0.444131 -5.496812\n",
       "1     -1   0.123043 -0.707875\n",
       "2     -1   0.113859 -0.931375\n",
       "3     -1   0.115371 -0.386000\n",
       "4     -1   0.102281 -0.378812"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possible_C = np.array([0.0001, 0.001, 0.01, 0.1, 1])\n",
    "runs = 100\n",
    "\n",
    "best_run_C = np.zeros(runs)\n",
    "C_val_errors = {c: np.zeros(runs) for c in possible_C}\n",
    "\n",
    "for run in range(runs):\n",
    "    kf = KFold(len(filtered), n_folds=10, shuffle=True)\n",
    "    C_errors = np.zeros(len(possible_C))\n",
    "    for i,C in enumerate(possible_C):\n",
    "        fold_errors = np.zeros(len(kf))\n",
    "        for j, index_pair in enumerate(kf):\n",
    "            train_index, val_index = index_pair\n",
    "            \n",
    "            train_fold = filtered.ix[train_index]\n",
    "            train_x = train_fold[[\"intensity\", \"symmetry\"]].values.tolist()\n",
    "            train_y = train_fold[\"digit\"].values.tolist()\n",
    "            \n",
    "            val_fold = filtered.ix[val_index]\n",
    "            val_x = val_fold[[\"intensity\", \"symmetry\"]].values.tolist()\n",
    "            val_y = val_fold[\"digit\"].values.tolist()\n",
    "            \n",
    "            m = svm_train(train_y, train_x, '-q -t 1 -d 2 -c {} -r 1 -g 1'.format(C))\n",
    "            p_label, p_acc, p_val = svm_predict(val_y, val_x, m, \"-q\")\n",
    "            fold_errors[j] = (100-p_acc[0])/100.\n",
    "            \n",
    "        mean_fold_error = np.mean(fold_errors)\n",
    "        C_val_errors[C][run] = mean_fold_error\n",
    "        C_errors[i] = mean_fold_error\n",
    "    best_run_C[run] = possible_C[np.argmin(C_errors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the 1 versus 5 classifier with Q = 2. We use $E_{cv}$ to select $C \\in \\{0.0001, 0.001, 0.01, 0.1, 1\\}$. If there is a tie in $E_{cv}$ , select the smaller C . Within the 100 random runs, which of the following statements is correct?\n",
    "\n",
    "\n",
    "After training, the classifier is tested and the errors on the test fold are saved. Afterwards, we average all errors for specific $C$ values for each fold over all the runs. We then calculate which $C$ yielded the lowest error rate in each run. By then calculating which $C$ was the most number of times selected as the one with the lowest error rate, we know which $C$ value is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  0.001\n"
     ]
    }
   ],
   "source": [
    "ctr = Counter(best_run_C)\n",
    "best_c = ctr.most_common(1)[0][0]\n",
    "print(\"Best C: \", best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEZCAYAAACU3p4jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1187c3550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(len(ctr))\n",
    "sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(0))\n",
    "keys = [t[0] for t in sorted_ctr]\n",
    "values = [t[1] for t in sorted_ctr]\n",
    "plt.bar(X, values, align='center', width=0.5, color=\"black\")\n",
    "plt.xticks(X, keys)\n",
    "ymax = max(values) + 1\n",
    "plt.ylim(0, ymax)\n",
    "plt.title(\"Number of times each C is selected as the best\")\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"Times selected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest cross validation error is achieved when $C=0.001$. As a result, our answer to this question is B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Again, consider the 1 versus 5 classifier with $Q = 2$. For the winning selection\n",
    "in the previous problem, the average value of $E_{cv}$ over the 100 runs is closest to...\n",
    "\n",
    "In this question, we need to report the error of the results using the best C value. As already saved this in order to calculate which C yielded the best results, reporting this value is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation error when using C= 0.001 :  0.00477266046056\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross validation error when using C=\", best_c, \": \", np.mean(C_val_errors[best_c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The achieved cross validation error is closest to $0.005$, which means that our answer is C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF kernel\n",
    "For the next questions, a radial basis function (RBF) kernel is used. This kernel is of the following form:\n",
    "$$\n",
    "K(x_n,x_m)=e^{-\\gamma\\|x_n-x_m\\|^2}\n",
    "$$\n",
    "Here, $x_n$ and $x_m$ are again data points. Besides the different kernel, this approach is the same as with the support vector machine using the polynomial kernel which was explained earlier. $\\gamma$ is a parameter passed to the algorithm and influences the variance of the obtained function. Each data point has an influence on the hypothesis based on its radial distance $\\|x-x_n\\|$. Intuitively, this means that nearby points have more influence than data points that are farther away. A small $\\gamma$ means that a single data point has a lot of influence and leads to low bias and high variance models, while a high $\\gamma$ leads to less influence and will give you a higher bias and lower variance.\n",
    "\n",
    "Notice that the corresponding non-linear transform ($\\phi$) of the kernel generates infinite-dimensional data points. As such, first computing the transform and then applying the inner product of 2 points would not be possible.\n",
    "\n",
    "For the following questions, we always use $\\gamma=1$ and we use the soft margin approach which was explained earlier. The experiments then involve trying different values for C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_test = test[(test.digit == 1) | (test.digit == 5)]\n",
    "filtered_test[\"digit\"].replace(1,float(-1),inplace=True)\n",
    "filtered_test[\"digit\"].replace(5,float(1),inplace=True)\n",
    "filtered_test = filtered_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possible_C = [0.01, 1, 100, 10**4, 10**6]\n",
    "E_ins = np.zeros(len(possible_C))\n",
    "E_outs = np.zeros(len(possible_C))\n",
    "\n",
    "for i,C in enumerate(possible_C):\n",
    "    train_x = filtered[[\"intensity\", \"symmetry\"]].values.tolist()\n",
    "    train_y = filtered[\"digit\"].values.tolist()\n",
    "    \n",
    "    test_x = filtered_test[[\"intensity\", \"symmetry\"]].values.tolist()\n",
    "    test_y = filtered_test[\"digit\"].values.tolist()\n",
    "    \n",
    "    m = svm_train(train_y, train_x, '-q -t 2 -d 2 -c {} -g 1'.format(C))\n",
    "    train_label, train_acc, train_val = svm_predict(train_y, train_x, m, \"-q\")\n",
    "    test_label, test_acc, test_val = svm_predict(test_y, test_x, m, \"-q\")\n",
    "    \n",
    "    E_ins[i] = (100-train_acc[0])/100.\n",
    "    E_outs[i] = (100-test_acc[0])/100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Which of the following values of C results in the lowest $E_{in}$?\n",
    "\n",
    "For this question, we had to report the C value which yielded the lowest in-sample error. To do this, we train for each value of C the SVM with that particular C on all data (of digits 1 and 5) and use this classifier on the training and test data (of digits 1 and 5). The errors on the training data and test data are then saved to separate lists.\n",
    "We then report the value of C with the lowest corresponding error in the list of in-sample errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_C[np.argmin(E_ins)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoYkqRMDQ5LUiYEhSerEwJAkdfL/AYOZsR5zeGUxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118629208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(len(E_ins))\n",
    "plt.bar(X, E_ins, align='center', width=0.5, color=\"black\")\n",
    "plt.xticks(X, possible_C)\n",
    "ymax = max(E_ins) + 0.001\n",
    "plt.ylim(0, ymax)\n",
    "plt.title(\"In-sample error for different values of C\")\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get the lowest in-sample error when $C=1000000$. This means that the answer to this question is E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Which of the following values of C results in the lowest $E_{out}$?\n",
    "\n",
    "Here, we need to report the C value that gave us the lowest out-of-sample error. As already mentioned, the errors on the test data were already saved to a list. Thus, we now need to report the value of C with the lowest corresponding error in this list of out-of-sample errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_C[np.argmin(E_outs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEZCAYAAACEkhK6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1186302b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(len(E_outs))\n",
    "plt.bar(X, E_outs, align='center', width=0.5, color=\"black\")\n",
    "plt.xticks(X, possible_C)\n",
    "ymax = max(E_outs) + 0.001\n",
    "plt.ylim(0, ymax)\n",
    "plt.title(\"Out-of-sample error for different values of C\")\n",
    "plt.xlabel(\"C value\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The lowest out-of-sample error is achieved when $C=100$. This is equal to the value of answer C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
