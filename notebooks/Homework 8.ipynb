{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "For this homework, the library _libsvm_ is needed. The easiest way to install this on _Mac OS X_ is using [_Homebrew_](http://brew.sh/) with the following command: `brew install libsvm`.\n",
    "\n",
    "The following questions from this homework will be answered and its solutions will be explained: questions 7, 8, 9 and 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial kernel\n",
    "In the first 2 questions, we need to experiment with 10-fold cross-validation using a polynomial kernel. We will first explain the workings of support vector machines, after which we will look at the use of a polynomial kernel. Afterwards, we see why cross-validation is useful and how it works.\n",
    "\n",
    "The purpose of support vector machines is to find dichotomies with border margins that are as large as possible.\n",
    "As we saw in the lectures, we first need the distance from the separation to the nearest point to the hyperplane $w^Tx=0$, in which $w$ are weights and $x$ are data points. This was found to be $\\frac{1}{2}w^T w$.\n",
    "This is the value that needs to be maximized in order to get a margin as big as possible. This should also be subject to $|w^T x_n + b| \\geq 1$ (in which $b$ is the bias) for $n=1,2,\\dots,N$, because the distance from the separating hyperplane to its nearest point should be normalized to be 1.\n",
    "Using quadratic programming, we eventually become $\\alpha=\\alpha_1, \\alpha_2, \\dots, \\alpha_N$, which maximizes this value. When $\\alpha_n > 0$, we say that the data point $x_n$ is a support vector. The hyperplane that separates the points is completely determined by those support vectors and must lie between the support vectors of different target values.\n",
    "\n",
    "For data which is not linearly separable, a nonlinear transform is needed, in which a transformation (using the function $\\phi$) of the data is done from the $\\mathcal{X}$ to the $\\mathcal{Z}$ space, in which the data in the latter space is linearly separable.\n",
    "\n",
    "\n",
    "In a polynomial kernel, this transformation function $\\phi: \\mathcal{X} \\rightarrow \\mathcal{Z}$ is a polynomial of a certain order $Q$.\n",
    "There are however cases where the data is still not separable after a transformation. As a result, some points violate the margin ($|w^T x_n + b| \\geq 1$ is violated). The total violation was found to be $\\sum_{n=1}^N \\xi_n$. An order of this value (the value $C$) is added to the value that needs to be minimized using quadratic programming in order to allow some errors. The higher $C$, the more errors are allowed in separating the data. Intuitively, we see that increasing $C$ makes the decision surface more smooth and simple. A margin that uses this $C$ is called a soft margin.\n",
    "\n",
    "\n",
    "For the first 2 exercises, we use cross validation. In the slides, we first saw that using small partition of the training set to validate the learned hypothesis leads to a bad estimate because the points taken out of the training set ($K$ points out of $N$) for validation may not be representative for estimating the out-of-sample error, $E_{out}$. When, we take a large partition however, we get again an accurate validation error ($E_{val}$), but because the model is learned from a small number of data points, we have a bigger chance of getting a bad model for the data. Thus, we need to balance $K$.\n",
    "\n",
    "The optimal situation however would be to have $K$ both small and large, thus getting a good model and a good estimation of $E_{out}$. To achieve this, we use points one time for validating, and other times for training a model.\n",
    "We separate the $N$ datapoints into a number of folds, $F$. These number of folds could be as large as $N$ itself, in which $N$ iterations will take place and when only one point is used for validation.\n",
    "In cross-validation, $K=\\frac{N}{F}$, but these K points differ each time as a different fold is used for validation.\n",
    "The total cross-validation error will then be $E_{cv}=\\frac{1}{F}\\sum_{n=1}^F e_n$, in which $e_n=E_{val}(g_n^-)$ and $g_n^-$ is the model trained on $N-K$ datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the exercises, we only take data with as target value (digit) 1 or 5 and replace this target values by respectively -1 and 1.\n",
    "After listing all the possible values on $C$ that need to be used with the support vector machine, we do a number of runs. In each run, we separate the training data randomly into 10 folds. Then, for each value of $C$, we iterate 10 times. Each time take a different fold on which the classifier will be tested and the other 9 folds to train the classifier. This classifier is made using a support vector machine which is given the current value of $C$. After training, the classifier is tested and the errors on the test fold are saved. Afterwards, we average all errors for specific $C$ values for each fold over all the runs. We then calculate which $C$ yielded the lowest error rate in each run. By then calculating which $C$ was the most number of times selected as the one with the lowest error rate, we know which $C$ value is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from svmutil import *\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import KFold\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"features.train\", sep=\" +\", header=None, engine='python')\n",
    "train.columns = [\"digit\", \"intensity\", \"symmetry\"]\n",
    "test = pd.read_table(\"features.test\", sep=\" +\", header=None, engine='python')\n",
    "test.columns = [\"digit\", \"intensity\", \"symmetry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>digit</th>\n",
       "      <th>intensity</th>\n",
       "      <th>symmetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.341092</td>\n",
       "      <td>-4.528937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.444131</td>\n",
       "      <td>-5.496812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.231002</td>\n",
       "      <td>-2.886750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.200275</td>\n",
       "      <td>-3.534375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.291936</td>\n",
       "      <td>-4.352062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   digit  intensity  symmetry\n",
       "0      6   0.341092 -4.528937\n",
       "1      5   0.444131 -5.496812\n",
       "2      4   0.231002 -2.886750\n",
       "3      7   0.200275 -3.534375\n",
       "4      3   0.291936 -4.352062"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = train[(train.digit == 1) | (train.digit == 5)]\n",
    "filtered.loc[filtered.digit == 1, \"digit\"] = -1\n",
    "filtered.loc[filtered.digit == 5, \"digit\"] = 1\n",
    "filtered = filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>digit</th>\n",
       "      <th>intensity</th>\n",
       "      <th>symmetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.444131</td>\n",
       "      <td>-5.496812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.123043</td>\n",
       "      <td>-0.707875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.113859</td>\n",
       "      <td>-0.931375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.115371</td>\n",
       "      <td>-0.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.102281</td>\n",
       "      <td>-0.378812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   digit  intensity  symmetry\n",
       "0      1   0.444131 -5.496812\n",
       "1     -1   0.123043 -0.707875\n",
       "2     -1   0.113859 -0.931375\n",
       "3     -1   0.115371 -0.386000\n",
       "4     -1   0.102281 -0.378812"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possible_C = np.array([0.0001, 0.001, 0.01, 0.1, 1])\n",
    "runs = 100\n",
    "\n",
    "best_run_C = np.zeros(runs)\n",
    "C_val_errors = {c: np.zeros(runs) for c in possible_C}\n",
    "\n",
    "for run in range(runs):\n",
    "    kf = KFold(len(filtered), n_folds=10, shuffle=True)\n",
    "    C_errors = np.zeros(len(possible_C))\n",
    "    for i,C in enumerate(possible_C):\n",
    "        fold_errors = np.zeros(len(kf))\n",
    "        for j, index_pair in enumerate(kf):\n",
    "            train_index, val_index = index_pair\n",
    "            \n",
    "            train_fold = filtered.ix[train_index]\n",
    "            train_x = train_fold[[\"intensity\", \"symmetry\"]].values.tolist()\n",
    "            train_y = train_fold[\"digit\"].values.tolist()\n",
    "            \n",
    "            val_fold = filtered.ix[val_index]\n",
    "            val_x = val_fold[[\"intensity\", \"symmetry\"]].values.tolist()\n",
    "            val_y = val_fold[\"digit\"].values.tolist()\n",
    "            \n",
    "            m = svm_train(train_y, train_x, '-q -t 1 -d 2 -c {} -r 1 -g 1'.format(C))\n",
    "            p_label, p_acc, p_val = svm_predict(val_y, val_x, m, \"-q\")\n",
    "            fold_errors[j] = (100-p_acc[0])/100.\n",
    "            \n",
    "        mean_fold_error = np.mean(fold_errors)\n",
    "        C_val_errors[C][run] = mean_fold_error\n",
    "        C_errors[i] = mean_fold_error\n",
    "    best_run_C[run] = possible_C[np.argmin(C_errors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C:  0.001\n"
     ]
    }
   ],
   "source": [
    "best_c = Counter(best_run_C).most_common(1)[0][0]\n",
    "print(\"Best C: \", best_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest cross validation error is achieved when $C=0.001$. As a result, our answer to this question is B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "In this question, we need to report the error of the results using the best C value. As already saved this in order to calculate which C yielded the best results, reporting this value is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation error when using C= 0.001 :  0.0046828352115\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross validation error when using C=\", best_c, \": \", np.mean(C_val_errors[best_c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The achieved cross validation error is closest to $0.005$, which means that our answer is C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF kernel\n",
    "For the next questions, a radial basis function (RBF) is used. This function is of the form:\n",
    "$$\n",
    "h(x) = \\sum_{n=1}^N w_n e^{-\\gamma \\| x - x_n \\|^2}\n",
    "$$\n",
    "The learning algorithm involves finding $w_n$. We find this values by minimizing $(h(x) - y)^2$, where $y$ is the target value. $\\gamma$ is a parameter passed to the algorithm and influences the variance of the obtained function. A small $\\gamma$ means that a single data point has a lot of influence and leads to high variance, while a high $\\gamma$ leads to less influence and a higher bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_test = test[(test.digit == 1) | (test.digit == 5)]\n",
    "filtered_test[\"digit\"].replace(1,float(-1),inplace=True)\n",
    "filtered_test[\"digit\"].replace(5,float(1),inplace=True)\n",
    "filtered_test = filtered_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "possible_C = [0.01, 1, 100, 10**4, 10**6]\n",
    "E_ins = np.zeros(len(possible_C))\n",
    "E_outs = np.zeros(len(possible_C))\n",
    "\n",
    "for i,C in enumerate(possible_C):\n",
    "    train_x = filtered[[\"intensity\", \"symmetry\"]].values.tolist()\n",
    "    train_y = filtered[\"digit\"].values.tolist()\n",
    "    \n",
    "    test_x = filtered_test[[\"intensity\", \"symmetry\"]].values.tolist()\n",
    "    test_y = filtered_test[\"digit\"].values.tolist()\n",
    "    \n",
    "    m = svm_train(train_y, train_x, '-q -t 2 -d 2 -c {} -g 1'.format(C))\n",
    "    train_label, train_acc, train_val = svm_predict(train_y, train_x, m, \"-q\")\n",
    "    test_label, test_acc, test_val = svm_predict(test_y, test_x, m, \"-q\")\n",
    "    \n",
    "    E_ins[i] = (100-train_acc[0])/100.\n",
    "    E_outs[i] = (100-test_acc[0])/100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "For this question, we had to report the C value which yielded the lowest in-sample error. To do this, we train for each value of C the SVM with that particular C on all data (of digits 1 and 5) and use this classifier on the training and test data (of digits 1 and 5). The errors on the training data and test data are then saved to separate lists.\n",
    "We then report the value of C with the lowest corresponding error in the list of in-sample errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_C[np.argmin(E_ins)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get the lowest in-sample error when $C=1000000$. This means that the answer to this question is E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Here, we need to report the C value that gave us the lowest out-of-sample error. As already mentioned, the errors on the test data were already saved to a list. Thus, we now need to report the value of C with the lowest corresponding error in this list of out-of-sample errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_C[np.argmin(E_outs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The lowest out-of-sample error is achieved when $C=100$. This is equal to the value of answer C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
